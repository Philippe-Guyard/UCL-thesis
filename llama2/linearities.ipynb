{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The idea of this notebook is to study how linear certain layers are. Inspired by [\"Your transformer is secretly linear\"](https://arxiv.org/pdf/2405.12250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from typing import Callable, Optional\n",
    "from dataclasses import dataclass\n",
    "\n",
    "from opt_sim_dataset import ConsecutiveOutputsDataset \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize(x): \n",
    "    X = x - x.mean(dim=0, keepdim=True)\n",
    "    return X / X.norm()\n",
    "\n",
    "def get_A_est(X, Y):\n",
    "    U, S, Vh = torch.linalg.svd(X, full_matrices=False)\n",
    "    A_estimation = Vh.T * (1 / S)[None, ...] @ U.T @ Y # Y=XA\n",
    "    return A_estimation\n",
    "\n",
    "def get_est_svd(X, Y):\n",
    "    \"\"\"\n",
    "    X -- torch tensor with shape [n_samples, dim]\n",
    "    Y -- torch tensor with shape [n_samples, dim]\n",
    "\n",
    "    Approximates Y matrix with linear transformation Y = XA\n",
    "    \"\"\"\n",
    "    A_estimation = get_A_est(X, Y) \n",
    "    Y_est =  X @ A_estimation\n",
    "    return Y_est\n",
    "\n",
    "def compute_linearity_score(x, y):\n",
    "    \"\"\"\n",
    "    x -- torch tensor with shape [n_samples, dim]\n",
    "    y -- torch tensor with shape [n_samples, dim]\n",
    "    \"\"\"\n",
    "    with torch.no_grad(): \n",
    "        X, Y = normalize(x), normalize(y)\n",
    "        Y_estimation = get_est_svd(X, Y)\n",
    "    \n",
    "        y_error = (Y_estimation - Y).square().sum()\n",
    "        sim = float(1 - y_error)\n",
    "    return sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = ConsecutiveOutputsDataset(Path('./data'), 22, 500, 0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_embeddings = [[] for _ in range(data.num_layers)]\n",
    "for x, v1, v2, blocks in data:\n",
    "    block = blocks[0]\n",
    "    block_embeddings[block].append(x.cpu())\n",
    "    if block == data.num_layers - 2:\n",
    "        block_embeddings[block + 1].append(v1.cpu())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "block_embeddings = [torch.cat(x).reshape(-1, 2048).to(device='cuda') for x in block_embeddings]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24706, 2048])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "block_embeddings[0].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.08292317390441895"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "compute_linearity_score(torch.randn_like(block_embeddings[0]), torch.randn_like(block_embeddings[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 1\n",
    "for idx in range(data.num_layers - k):\n",
    "    print(idx, compute_linearity_score(block_embeddings[idx], block_embeddings[idx + k]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_linearity_dist(block1, block2):\n",
    "    n_samples = block1.size(0)\n",
    "    assert n_samples == block2.size(0)\n",
    "    X, Y = normalize(block1), normalize(block2)\n",
    "    A_est = get_A_est(X, Y)\n",
    "    linearities = torch.zeros(n_samples)\n",
    "    for idx, (x, y) in enumerate(zip(X, Y)): \n",
    "        err = (A_est @ x - y).square().sum() \n",
    "        linearities[idx] = 1 - err \n",
    "\n",
    "    return linearities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.displot(get_linearity_dist(block_embeddings[2], block_embeddings[7]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleAutoEncoder(nn.Module):\n",
    "    def __init__(self, embed_dim: int, hidden_size: int):\n",
    "        super(SimpleAutoEncoder, self).__init__()\n",
    "\n",
    "        self.fc1 = nn.Linear(embed_dim, hidden_size)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(embed_dim, hidden_size)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.fc2(self.relu(self.fc1(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N = len(data)\n",
    "train_len = int(N * 0.7)\n",
    "train_data, test_data = random_split(data, lengths=[train_len, N - train_len])\n",
    "train_loader = DataLoader(train_data, batch_size=16, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=16, shuffle=False)\n",
    "tgt_block = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0, Train loss: 3.5611e-02, Test loss 6.3485e-03\n",
      "epoch=1, Train loss: 2.2423e-02, Test loss 3.3661e-03\n",
      "epoch=2, Train loss: 1.1353e-02, Test loss 2.0385e-03\n",
      "epoch=3, Train loss: 4.5377e-03, Test loss 6.0938e-04\n",
      "epoch=4, Train loss: 1.7525e-03, Test loss 2.8281e-04\n",
      "epoch=5, Train loss: 1.2634e-03, Test loss 1.2395e-04\n",
      "epoch=6, Train loss: 5.9697e-04, Test loss 1.4333e-04\n",
      "epoch=7, Train loss: 1.3739e-03, Test loss 8.2964e-05\n",
      "epoch=8, Train loss: 3.6785e-04, Test loss 7.7285e-05\n",
      "epoch=9, Train loss: 1.9110e-03, Test loss 8.7417e-05\n"
     ]
    }
   ],
   "source": [
    "criterion = nn.MSELoss()\n",
    "# model = SimpleAutoEncoder(768, 768).to('cuda')\n",
    "model = nn.Linear(768, 768).to('cuda')\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr=1e-3)\n",
    "for epoch in range(10):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for x, v1, v2, blocks in train_loader:\n",
    "        x_blocks = blocks[:, 0]\n",
    "        indices = x_blocks == tgt_block \n",
    "        if indices.sum() == 0:\n",
    "            continue\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(x[indices])\n",
    "        loss = criterion(outputs , v1[indices])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        epoch_loss += loss.item()\n",
    "    \n",
    "    test_loss = 0\n",
    "    model.eval()\n",
    "    for x, v1, v2, blocks in test_loader:\n",
    "        x_blocks = blocks[:, 0]\n",
    "        indices = x_blocks == tgt_block\n",
    "        if indices.sum() == 0:\n",
    "            continue\n",
    "    \n",
    "        outputs = model(x[indices])\n",
    "        loss = criterion(outputs, v1[indices])\n",
    "        test_loss += loss.item()\n",
    "\n",
    "    print(f'{epoch=}, Train loss: {epoch_loss / len(train_data):.4e}, Test loss {test_loss / len(test_data):.4e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(1.6440, device='cuda:0')\n",
      "1 tensor(2.5635, device='cuda:0')\n",
      "2 tensor(3.1261, device='cuda:0')\n",
      "3 tensor(4.0580, device='cuda:0')\n",
      "4 tensor(4.3789, device='cuda:0')\n",
      "5 tensor(4.8121, device='cuda:0')\n",
      "6 tensor(5.1682, device='cuda:0')\n",
      "7 tensor(5.6405, device='cuda:0')\n",
      "8 tensor(6.2163, device='cuda:0')\n",
      "9 tensor(7.1597, device='cuda:0')\n",
      "10 tensor(8.5492, device='cuda:0')\n",
      "11 tensor(10.3722, device='cuda:0')\n",
      "12 tensor(9.9927, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(block_embeddings)):\n",
    "    print(idx, block_embeddings[idx].norm(dim=-1).mean()) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.0503, device='cuda:0')"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "F.cosine_similarity(block_embeddings[0][0], block_embeddings[1][0], dim=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 tensor(0.7336, device='cuda:0')\n",
      "1 tensor(0.9431, device='cuda:0')\n",
      "2 tensor(0.9613, device='cuda:0')\n",
      "3 tensor(0.9715, device='cuda:0')\n",
      "4 tensor(0.9599, device='cuda:0')\n",
      "5 tensor(0.9502, device='cuda:0')\n",
      "6 tensor(0.9487, device='cuda:0')\n",
      "7 tensor(0.9480, device='cuda:0')\n",
      "8 tensor(0.9332, device='cuda:0')\n",
      "9 tensor(0.9221, device='cuda:0')\n",
      "10 tensor(0.9286, device='cuda:0')\n",
      "11 tensor(0.9055, device='cuda:0')\n"
     ]
    }
   ],
   "source": [
    "for idx in range(len(block_embeddings) - 1):\n",
    "    print(idx, F.cosine_similarity(block_embeddings[idx], block_embeddings[idx + 1], dim=-1).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.1.-1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
